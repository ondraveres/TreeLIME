{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avast-CTU Public CAPE Dataset (Model Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following document demonstrates usage of the dataset. We first show statistics of the dataset, following by creating an HMIL model build and trained based no the reduced reports.\n",
    "\n",
    "The example is using Julia 1.6, Mill.jl 2.7 and JsonGrinder 2.2.3. For details about julia or the main packages, please see the official documentations:\n",
    "* https://julialang.org\n",
    "* https://ctuavastlab.github.io/JsonGrinder.jl/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation of the environment, addiing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Diplomka/ExplainMill.jl/myscripts`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"./\")\n",
    "\n",
    "using Flux, MLDataPattern, Mill, JsonGrinder, JSON, Statistics, IterTools, StatsBase, ThreadTools\n",
    "using JsonGrinder: suggestextractor, ExtractDict\n",
    "using Mill: reflectinmodel\n",
    "using CSV, DataFrames\n",
    "using Random\n",
    "using Dates\n",
    "using Plots\n",
    "using Printf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parts of the code is parallelized to speed up loading / working with data. The following variable determines the number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THREADS = Threads.nthreads() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the path variables to directories containing metadata (labels) and reports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_REPORTS = \"PATH/TO/REPORTS/\"\n",
    "PATH_TO_REDUCED_REPORTS = PATH_TO_REPORTS * \"public_small_reports/\"\n",
    "PATH_TO_FULL_REPORTS = PATH_TO_REPORTS * \"public_full_reports/\"\n",
    "PATH_TO_LABELS = \"./\" ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: \"./public_labels.csv\" is not a valid file or doesn't exist",
     "output_type": "error",
     "traceback": [
      "ArgumentError: \"./public_labels.csv\" is not a valid file or doesn't exist\n",
      "\n",
      "Stacktrace:\n",
      " [1] CSV.Context(source::CSV.Arg, header::CSV.Arg, normalizenames::CSV.Arg, datarow::CSV.Arg, skipto::CSV.Arg, footerskip::CSV.Arg, transpose::CSV.Arg, comment::CSV.Arg, ignoreemptyrows::CSV.Arg, ignoreemptylines::CSV.Arg, select::CSV.Arg, drop::CSV.Arg, limit::CSV.Arg, buffer_in_memory::CSV.Arg, threaded::CSV.Arg, ntasks::CSV.Arg, tasks::CSV.Arg, rows_to_check::CSV.Arg, lines_to_check::CSV.Arg, missingstrings::CSV.Arg, missingstring::CSV.Arg, delim::CSV.Arg, ignorerepeated::CSV.Arg, quoted::CSV.Arg, quotechar::CSV.Arg, openquotechar::CSV.Arg, closequotechar::CSV.Arg, escapechar::CSV.Arg, dateformat::CSV.Arg, dateformats::CSV.Arg, decimal::CSV.Arg, groupmark::CSV.Arg, truestrings::CSV.Arg, falsestrings::CSV.Arg, stripwhitespace::CSV.Arg, type::CSV.Arg, types::CSV.Arg, typemap::CSV.Arg, pool::CSV.Arg, downcast::CSV.Arg, lazystrings::CSV.Arg, stringtype::CSV.Arg, strict::CSV.Arg, silencewarnings::CSV.Arg, maxwarnings::CSV.Arg, debug::CSV.Arg, parsingdebug::CSV.Arg, validate::CSV.Arg, streaming::CSV.Arg)\n",
      "   @ CSV ~/.julia/packages/CSV/aoJqo/src/context.jl:314\n",
      " [2] #File#32\n",
      "   @ ~/.julia/packages/CSV/aoJqo/src/file.jl:222 [inlined]\n",
      " [3] CSV.File(source::String)\n",
      "   @ CSV ~/.julia/packages/CSV/aoJqo/src/file.jl:162\n",
      " [4] read(source::String, sink::Type; copycols::Bool, kwargs::@Kwargs{})\n",
      "   @ CSV ~/.julia/packages/CSV/aoJqo/src/CSV.jl:117\n",
      " [5] read(source::String, sink::Type)\n",
      "   @ CSV ~/.julia/packages/CSV/aoJqo/src/CSV.jl:113\n",
      " [6] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "df_labels = CSV.read(PATH_TO_LABELS * \"public_labels.csv\", DataFrame) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for malware families in the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first provide some basic statistics. First, we show the list of malware families in the dataset and how many samples belong to each family. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `df_labels` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `df_labels` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "all_samples_count = size(df_labels, 1)\n",
    "println(\"All samples: $(all_samples_count)\")\n",
    "println(\"Malware families: \")\n",
    "[println(k => v) for (k,v) in countmap(df_labels.classification_family)] ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show how the sampels are distributed over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `df_labels` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `df_labels` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "df_labels[!,:month] = map(i -> string(year(i), \"-\", month(i) < 10 ? \"0$(month(i))\" : month(i)), df_labels.date) ;\n",
    "month_counts = sort(countmap(df_labels.month) |> collect, by = x -> x[1])\n",
    "index2017 = findfirst(j -> j[1] == \"2017-01\", month_counts)\n",
    "previous_months = sum(map(j -> j[2], month_counts[1:index2017-1]))\n",
    "month_counts[index2017] = Pair(\"≤\"*month_counts[index2017][1], month_counts[index2017][2]+previous_months)\n",
    "deleteat!(month_counts, 1:64)\n",
    "bar(getindex.(month_counts,2), xticks=(1:length(month_counts), getindex.(month_counts,1)), xtickfontsize=5, ytickfontsize=5, xrotation=45, yguidefontsize=8, xguidefontsize=8, legend=false,\n",
    "    xlabel=\"Month and year of the first evidence of a sample\", ylabel=\"Number of samples for each month\",size=(900,400),\n",
    "    left_margin = 5Plots.mm, bottom_margin = 10Plots.mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to split data into the training and testing part. We do this according the time to reflect the way in which the models should be used (i.e., to detect new, unseen malware). We are using date **2019-08-01** as an example splitting date, however, other dates can be used in a more detailed study of the drift and changes in the data distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `all_samples_count` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `all_samples_count` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:2"
     ]
    }
   ],
   "source": [
    "timesplit = Date(2019,8,1)\n",
    "train_indexes = findall(i -> df_labels.date[i] < timesplit, 1:all_samples_count)\n",
    "test_indexes = [setdiff(Set(1:all_samples_count), Set(train_indexes))...] ;\n",
    "\n",
    "train_size = length(train_indexes)\n",
    "test_size = length(test_indexes)\n",
    "\n",
    "println(\"Train size: $(train_size)\")\n",
    "println(\"Test size: $(test_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load all JSON files. For the example model, we are using reduced reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `df_labels` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `df_labels` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "jsons = tmap(df_labels.sha256) do s\n",
    "    try \n",
    "        open(JSON.parse, \"$(PATH_TO_REDUCED_REPORTS)$(s).json\")\n",
    "    catch e\n",
    "        @error \"Error when processing sha $s: $e\"\n",
    "    end\n",
    "end ;\n",
    "@assert size(jsons, 1) == all_samples_count # verifying that all samples loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Next, we are going to build schema out of the JSONs and model corresponding to these JSONs. Note that we are using only training data to build the schema and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `train_indexes` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `train_indexes` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "chunks = Iterators.partition(train_indexes, div(train_size, THREADS))\n",
    "sch_parts = tmap(chunks) do ch\n",
    "    JsonGrinder.schema(jsons[ch])\n",
    "end\n",
    "time_split_complete_schema = merge(sch_parts...)\n",
    "printtree(time_split_complete_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the JSONs based on the scheme so that we can build and train the model. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `time_split_complete_schema` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `time_split_complete_schema` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "extractor = suggestextractor(time_split_complete_schema)\n",
    "data = tmap(extractor, jsons) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for creating the model, prepare minibatches and callback functions for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `df_labels` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `df_labels` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "labelnames = sort(unique(df_labels.classification_family))\n",
    "neurons = 32\n",
    "model = reflectinmodel(time_split_complete_schema, extractor,\n",
    "\tk -> Dense(k, neurons, relu),\n",
    "\td -> SegmentedMeanMax(d),\n",
    "\tfsm = Dict(\"\" => k -> Dense(k, length(labelnames))),\n",
    ")\n",
    "\n",
    "minibatchsize = 500\n",
    "function minibatch()\n",
    "\tidx = sample(train_indexes, minibatchsize, replace = false)\n",
    "\treduce(catobs, data[idx]), Flux.onehotbatch(df_labels.classification_family[idx], labelnames)\n",
    "end\n",
    "\n",
    "iterations = 200\n",
    "\n",
    "function accuracy(x,y) \n",
    "    vals = tmap(x) do s\n",
    "        Flux.onecold(softmax(model(s)), labelnames)[1]\n",
    "    end\n",
    "    mean(vals .== y)\n",
    "end     \n",
    "    \n",
    "\n",
    "eval_trainset = shuffle(train_indexes)[1:1000]\n",
    "eval_testset = shuffle(test_indexes)[1:1000]\n",
    "\n",
    "cb = () -> begin\n",
    "\ttrain_acc = accuracy(data[eval_trainset], df_labels.classification_family[eval_trainset])\n",
    "\ttest_acc = accuracy(data[eval_testset], df_labels.classification_family[eval_testset])\n",
    "\tprintln(\"accuracy: train = $train_acc, test = $test_acc\")\n",
    "end\n",
    "ps = Flux.params(model)\n",
    "loss = (x,y) -> Flux.logitcrossentropy(model(x), y)\n",
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model (this can take some time). Note that actual performance may slightly vary from the numbers presented in accompanying paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `minibatch` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `minibatch` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "Flux.Optimise.train!(loss, ps, repeatedly(minibatch, iterations), opt, cb = Flux.throttle(cb, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained, we can evaluate the performance on the complete test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `data` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `data` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "full_test_accuracy = accuracy(data[test_indexes], df_labels.classification_family[test_indexes])\n",
    "println(\"Final evaluation:\")\n",
    "println(\"Accuratcy on test data: $(full_test_accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the confusion matrix table of the testing data for different malware families. The true lables are in the row, the predictions are in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `labelnames` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `labelnames` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:2"
     ]
    }
   ],
   "source": [
    "test_predictions = Dict()\n",
    "for true_label in labelnames\n",
    "    current_predictions = Dict()\n",
    "    [current_predictions[pl]=0.0 for pl in labelnames]\n",
    "    family_indexes = filter(i -> df_labels.classification_family[i] == true_label, test_indexes)\n",
    "    predictions = tmap(data[family_indexes]) do s\n",
    "        Flux.onecold(softmax(model(s)), labelnames)[1]\n",
    "    end\n",
    "    [current_predictions[pl] += 1.0 for pl in predictions]\n",
    "    [current_predictions[pl] = current_predictions[pl] ./ length(predictions) for pl in labelnames]\n",
    "    test_predictions[true_label] = current_predictions\n",
    "end\n",
    "\n",
    "@printf \"%8s\\t\" \"TL\\\\PL\"\n",
    "[@printf \" %8s\" s for s in labelnames]\n",
    "print(\"\\n\")\n",
    "for tl in labelnames\n",
    "    @printf \"%8s\\t\" tl \n",
    "    for pl in labelnames\n",
    "        @printf \"%9s\" @sprintf \"%.2f\" test_predictions[tl][pl]*100\n",
    "    end\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test the static version of the reduced model, the schema can be altered to remove the behavioral part. Re-extracting the data with the new extractor and re-training the model would work in the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `time_split_complete_schema` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `time_split_complete_schema` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Diplomka/ExplainMill.jl/myscripts/capev2.ipynb:1"
     ]
    }
   ],
   "source": [
    "time_split_static_schema = deepcopy(time_split_complete_schema)\n",
    "delete!(time_split_static_schema.childs,:behavior)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
